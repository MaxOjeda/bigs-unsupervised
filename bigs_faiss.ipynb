{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a95de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs_ssd/mojeda_imfd/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import sys\n",
    "import resource\n",
    "import faiss\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2447ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- tiny memory helpers (simple & cross-platform-ish) ----\n",
    "def _ru_maxrss_mb() -> float:\n",
    "    ru = resource.getrusage(resource.RUSAGE_SELF)\n",
    "    # Linux: KB, macOS/BSD: bytes\n",
    "    if sys.platform.startswith(\"linux\"):\n",
    "        return ru.ru_maxrss / 1024.0\n",
    "    return ru.ru_maxrss / (1024.0 * 1024.0)\n",
    "\n",
    "def _rss_now_mb() -> float:\n",
    "    try:\n",
    "        import psutil\n",
    "        return psutil.Process().memory_info().rss / (1024.0 * 1024.0)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def _measure(callable_fn, *args, **kwargs):\n",
    "    \"\"\"Run callable_fn(*args, **kwargs) and return (result, elapsed_s, mem_delta_mb).\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    ru0 = _ru_maxrss_mb()\n",
    "    rss0 = _rss_now_mb()\n",
    "    result = callable_fn(*args, **kwargs)\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    mem_delta = max(0.0, _ru_maxrss_mb() - ru0, _rss_now_mb() - rss0)\n",
    "    return result, elapsed, mem_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ab76d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/textualization/japan/original/gpt-4o-mini.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m serialized_triples_list = []\n\u001b[32m      2\u001b[39m sentences_list = []\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/textualization/japan/original/gpt-4o-mini.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(f, desc=\u001b[33m\"\u001b[39m\u001b[33mReading lines\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      6\u001b[39m         obj = json.loads(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/textualization/japan/original/gpt-4o-mini.txt'"
     ]
    }
   ],
   "source": [
    "serialized_triples_list = []\n",
    "sentences_list = []\n",
    "\n",
    "with open(\"data/textualization/japan/original/gpt-4o-mini.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Reading lines\"):\n",
    "        obj = json.loads(line)\n",
    "        temp_triples = [' '.join(triples) for triples in obj['triples']]\n",
    "        serialized_triples_list.append(temp_triples)\n",
    "        sentences_list.append(obj[\"sentence\"])\n",
    "serialized_triples_list = [text for ls in serialized_triples_list for text in ls]\n",
    "print(f\"Collected {len(serialized_triples_list)} items.\")\n",
    "for i in range(min(3, len(serialized_triples_list))):\n",
    "    print(f\"- serialized_triples: {serialized_triples_list[i]}\")\n",
    "    print(f\"  sentence          : {sentences_list[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156340f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading lines: 100000it [00:00, 170503.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 170210 items.\n",
      "- serialized_triples: Mikhail Belyaev date of death 01 January 1918\n",
      "  sentence          : Mikhail Alekseyevich Belyaev (Russian: ; December 23, 1863Â - 1918) was a Russian general of the Infantry, statesman, Chief of Staff of the Imperial Russian Army from August 1, 1914 to August 10, 1916, and was the last Minister of War of the Russian Empire from January 3, 1917 to February 28, 1917.\n",
      "- serialized_triples: Mikhail Belyaev allegiance Russian Empire\n",
      "  sentence          : Shiels Jewellers is an Australian jewellery retailer and was founded by Jack Shiels in Adelaide in 1945.\n",
      "- serialized_triples: Mikhail Belyaev position held minister of war\n",
      "  sentence          : The framing story is set in the 21st century and follows Desmond Miles as Assassin 's Creed II relives the genetic memories of his ancestor Ezio Auditore da Firenze.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def parse_tsv_json_lines(data_path: Path):\n",
    "    \"\"\"Cada línea del archivo es un JSON con 'serialized_triples' y 'sentence'.\"\"\"\n",
    "    serialized_triples_list = []\n",
    "    sentences_list = []\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Reading lines\"):\n",
    "            obj = json.loads(line)\n",
    "            serialized_triples_list.append(obj[\"serialized_triples\"])\n",
    "            sentences_list.append(obj[\"sentence\"])\n",
    "\n",
    "    print(f\"Collected {len(serialized_triples_list)} items.\")\n",
    "    for i in range(min(3, len(serialized_triples_list))):\n",
    "        print(f\"- serialized_triples: {serialized_triples_list[i]}\")\n",
    "        print(f\"  sentence          : {sentences_list[i]}\")\n",
    "\n",
    "    return serialized_triples_list, sentences_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c30c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _validate_finite(name: str, x: np.ndarray) -> None:\n",
    "    if not np.isfinite(x).all():\n",
    "        bad_rows = np.unique(np.argwhere(~np.isfinite(x))[:, 0])[:10]\n",
    "        raise ValueError(f\"{name}: found non-finite values; first bad rows: {bad_rows}\")\n",
    "\n",
    "def generate_embeddings(\n",
    "    originals: list[str],\n",
    "    generated: list[str],\n",
    "    model: SentenceTransformer,\n",
    "    batch_size: int = 64,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    print(\"Generating embeddings...\")\n",
    "    orig_emb = model.encode(\n",
    "        originals, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True\n",
    "    )\n",
    "    gen_emb = model.encode(\n",
    "        generated, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True\n",
    "    )\n",
    "    print(\"Embeddings generated.\")\n",
    "    return orig_emb, gen_emb\n",
    "\n",
    "\n",
    "def bigs_scores_hnsw(\n",
    "    original_embeddings: np.ndarray,\n",
    "    generated_embeddings: np.ndarray,\n",
    "    hnsw_M: int = 16,\n",
    "    ef_construction: int = 200,\n",
    "    ef_search: int = 128,\n",
    ") -> tuple[float, float, float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Cosine distance via L2-normalized vectors + L2 metric (d2 = 2*(1-cos); cos_dist = d2/2).\n",
    "    returns: (score_r, score_r_std, score_r_med, score_l, score_l_std, score_l_med, encode_elapsed)\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.ascontiguousarray(original_embeddings, dtype=\"float32\")\n",
    "    Y = np.ascontiguousarray(generated_embeddings, dtype=\"float32\")\n",
    "\n",
    "    if X.ndim != 2 or Y.ndim != 2 or X.shape[1] != Y.shape[1]:\n",
    "        raise ValueError(\"Embeddings must be 2D and have matching dimensions.\")\n",
    "\n",
    "    _validate_finite(\"X (norm)\", X)\n",
    "    _validate_finite(\"Y (norm)\", Y)\n",
    "\n",
    "    print(\"X min/max:\", X.min(), X.max(), \"Y min/max:\", Y.min(), Y.max())\n",
    "\n",
    "    d = X.shape[1]\n",
    "\n",
    "    # en macbook m1 necesitaba agregar esto o daba error:\n",
    "    #try:\n",
    "    #    faiss.omp_set_num_threads(1)\n",
    "    #except Exception:\n",
    "    #    pass\n",
    "\n",
    "    print(\"Calculating BIGS -> ...\")\n",
    "\n",
    "    # --- Right: document -> graph ---\n",
    "    idx_r = faiss.IndexHNSWFlat(d, hnsw_M)  # L2, devuelve distancias L2^2\n",
    "    idx_r.hnsw.efConstruction = ef_construction\n",
    "    idx_r.hnsw.efSearch = ef_search\n",
    "\n",
    "    idx_r.add(Y)  # index sobre Y\n",
    "    Dr, _ = idx_r.search(X, 1)  # (n_orig, 1) L2^2\n",
    "    right_min = Dr[:, 0] / 2.0  # cos_dist\n",
    "\n",
    "    print(\"Calculating BIGS <- ...\")\n",
    "    # --- Left: graph -> document ---\n",
    "    idx_l = faiss.IndexHNSWFlat(d, hnsw_M)\n",
    "    idx_l.hnsw.efConstruction = ef_construction\n",
    "    idx_l.hnsw.efSearch = ef_search\n",
    "\n",
    "    idx_l.add(X)  # index sobre X\n",
    "    Dl, _ = idx_l.search(Y, 1)  # (n_gen, 1) L2^2\n",
    "    left_min = Dl[:, 0] / 2.0  # cos_dist\n",
    "\n",
    "    # Stats\n",
    "    score_r = float(right_min.mean())\n",
    "    score_r_std = float(right_min.std())\n",
    "    score_r_med = float(np.median(right_min))\n",
    "\n",
    "    score_l = float(left_min.mean())\n",
    "    score_l_std = float(left_min.std())\n",
    "    score_l_med = float(np.median(left_min))\n",
    "\n",
    "    return (score_r, score_r_std, score_r_med, score_l, score_l_std, score_l_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622cea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_for_tsv(\n",
    "    tsv_path: Path,\n",
    "    model_name: str,\n",
    "    *,\n",
    "    batch_size: int = 64,\n",
    "    hnsw_M: int = 16,\n",
    "    ef_construction: int = 200,\n",
    "    ef_search: int = 128,\n",
    ") -> dict[str, object]:\n",
    "    \"\"\"Parse -> embed (timed/mem) -> BIGS (timed/mem). Returns a dict with all stats.\"\"\"\n",
    "    # 1) Parse\n",
    "    serialized_triples, sentences = parse_tsv_json_lines(tsv_path)\n",
    "    n = len(serialized_triples)\n",
    "\n",
    "    # 2) Build model and generate embeddings (timed/mem)\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    (embeds, enc_time, enc_mem) = _measure(\n",
    "        generate_embeddings,\n",
    "        serialized_triples, sentences, model,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    orig_emb, gen_emb = embeds\n",
    "    d = int(orig_emb.shape[1])\n",
    "\n",
    "    # 3) BIGS with FAISS HNSW (timed/mem)\n",
    "    (bigs, search_time, search_mem) = _measure(\n",
    "        bigs_scores_hnsw,\n",
    "        orig_emb, gen_emb,\n",
    "        hnsw_M=hnsw_M,\n",
    "        ef_construction=ef_construction,\n",
    "        ef_search=ef_search,\n",
    "    )\n",
    "    (score_r, score_r_std, score_r_med,\n",
    "     score_l, score_l_std, score_l_med) = bigs\n",
    "\n",
    "\n",
    "    # # 3) BIGS Normal\n",
    "    # (bigs, search_time, search_mem) = _measure(\n",
    "    #     bigs_scores_normal,\n",
    "    #     orig_emb, gen_emb\n",
    "    # )\n",
    "    # (score_r, score_r_std, score_r_med,\n",
    "    #  score_l, score_l_std, score_l_med) = bigs\n",
    "\n",
    "    # 4) Package results\n",
    "    return {\n",
    "        # dataset / model\n",
    "        \"file\": tsv_path.name,\n",
    "        \"samples\": n,\n",
    "        \"model_name\": model_name,\n",
    "        \"embedding_dim\": d,\n",
    "        \"batch_size\": batch_size,\n",
    "        # hnsw params\n",
    "        \"hnsw_M\": hnsw_M,\n",
    "        \"ef_construction\": ef_construction,\n",
    "        \"ef_search\": ef_search,\n",
    "        # timings\n",
    "        \"encode_time_s\": round(enc_time, 4),\n",
    "        \"search_time_s\": round(search_time, 4),\n",
    "        \"total_time_s\": round(enc_time + search_time, 4),\n",
    "        # memory (best-effort process delta)\n",
    "        \"encode_mem_delta_mb\": round(enc_mem, 2),\n",
    "        \"search_mem_delta_mb\": round(search_mem, 2),\n",
    "        # BIGS stats\n",
    "        \"bigs_r_mean\": round(score_r, 6),\n",
    "        \"bigs_r_std\": round(score_r_std, 6),\n",
    "        \"bigs_r_med\": round(score_r_med, 6),\n",
    "        \"bigs_l_mean\": round(score_l, 6),\n",
    "        \"bigs_l_std\": round(score_l_std, 6),\n",
    "        \"bigs_l_med\": round(score_l_med, 6),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2efd9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "japan = Path(\"data/triplet_lists/japan/langchain/gpt-4o-mini.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_50k = run_experiment_for_tsv(\n",
    "    japan,\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",  # \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    batch_size=256,\n",
    "    hnsw_M=16, ef_construction=200, ef_search=128,\n",
    ")\n",
    "print(results_50k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
